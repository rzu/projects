{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import display\n",
    "import pandas as pd\n",
    "import time, json, random, re, datetime\n",
    "import configparser as cp \n",
    "\n",
    "class InstagramScraper():\n",
    "    def __init__(self):\n",
    "        self.config = cp.ConfigParser()\n",
    "        self.config.read('config.cfg')\n",
    "        \n",
    "        # Dummy account credentials\n",
    "        self.username = self.config.get('core', 'username')\n",
    "        self.password = self.config.get('core', 'password')\n",
    "        \n",
    "        # Change to whatever driver is preferred\n",
    "        self.driver = webdriver.Chrome() \n",
    "        \n",
    "        self.today = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "        \n",
    "    def parse_json(self, page):\n",
    "        # Create a bs4 object from page source\n",
    "        soup = BeautifulSoup(page)\n",
    "        \n",
    "        # Grab interesting JSON only\n",
    "        json_main = soup.find('script', string = re.compile(\"^window._sharedData\") ).string.replace('window._sharedData = ', '')[:-1]\n",
    "        \n",
    "        # Parse the JSON into a dictionary\n",
    "        parsed = json.loads(json_main)\n",
    "        \n",
    "        return(parsed)\n",
    "    \n",
    "    def remove_emojis(self, string):\n",
    "        # Remove unwanted unicode emojies and special characters by forcing latin-1\n",
    "        return(string.encode('latin-1', 'ignore').decode('utf-8'))\n",
    "        \n",
    "    def do_login(self):\n",
    "        # Load page\n",
    "        self.driver.get(\"https://www.instagram.com/accounts/login/\")\n",
    "        \n",
    "        # Wait a bit for the page to load\n",
    "        time.sleep(2)\n",
    "\n",
    "        # Input the credentials\n",
    "        self.driver.find_element_by_xpath(\"//input[@name='username']\").send_keys(self.username)\n",
    "        self.driver.find_element_by_xpath(\"//input[@name='password']\").send_keys(self.password)\n",
    "\n",
    "        # Send\n",
    "        self.driver.find_element_by_xpath(\"//button[@type='submit']\").click()\n",
    "        \n",
    "        # Wait a bit for the page to load after logging in\n",
    "        time.sleep(2)\n",
    "        \n",
    "        return(True)\n",
    "        \n",
    "    # Scrape account metadata, image lists and followers\n",
    "    def scrape_accounts(self, accounts = list(), do_profiles = True, do_posts = False, do_followers = False):\n",
    "        # Prepare empty results lists\n",
    "        profiles  = list()\n",
    "        posts     = list()\n",
    "        followers = list()\n",
    "        \n",
    "        # Cycle through target list and get metadata\n",
    "        for one in accounts:\n",
    "            self.driver.get('https://www.instagram.com/{0}/'.format(one))\n",
    "            \n",
    "            # check if the request returned 200\n",
    "            if 'dialog-404' in self.driver.page_source:\n",
    "                continue\n",
    "            \n",
    "            # wait a bit while the page loads\n",
    "            time.sleep(2)\n",
    "            \n",
    "            # Get the profile metadata\n",
    "            metadata = self.get_account_metadata(self.driver.page_source)\n",
    "            profiles.append(metadata)\n",
    "            \n",
    "            if not metadata['is_private'] and do_posts:\n",
    "                posts += self.get_posts(metadata)\n",
    "            if not metadata['is_private'] and do_followers:\n",
    "                followers.append(self.get_followers(metadata))\n",
    "            \n",
    "            # Randomized delay between requests\n",
    "            time.sleep(random.randint(1,3))\n",
    "        \n",
    "        # Write this to a .csv file\n",
    "        if do_profiles:\n",
    "            display(pd.DataFrame(profiles))\n",
    "            pd.DataFrame(profiles).to_csv('account_metadata/{0}.csv'.format(self.today.replace('-', '')), index=False)\n",
    "        if not metadata['is_private'] and do_posts:\n",
    "            df = pd.DataFrame(posts)\n",
    "            df.drop_duplicates(inplace=True)\n",
    "            display(df)\n",
    "            df.to_csv('account_posts/{0}.csv'.format(self.today.replace('-', '')), index=False)\n",
    "        if not metadata['is_private'] and do_followers:\n",
    "            pd.DataFrame(followers).to_csv('account_followers/{0}.csv'.format(self.today.replace('-', '')), index=False)\n",
    "            \n",
    "    # Parse account metadata from html/json\n",
    "    def get_account_metadata(self, page):\n",
    "        # Parse the page JSON into a dictionary\n",
    "        parsed = self.parse_json(page)\n",
    "        \n",
    "        metadata = {\n",
    "            'date'      : self.today,\n",
    "            'username'  : parsed['entry_data']['ProfilePage'][0]['graphql']['user']['username'],\n",
    "            'full_name' : self.remove_emojis(parsed['entry_data']['ProfilePage'][0]['graphql']['user']['full_name']),\n",
    "            'is_private': 1 if parsed['entry_data']['ProfilePage'][0]['graphql']['user']['is_private'] else 0,\n",
    "            'is_business_account': 1 if parsed['entry_data']['ProfilePage'][0]['graphql']['user']['is_business_account'] else 0,\n",
    "            'external_url': parsed['entry_data']['ProfilePage'][0]['graphql']['user']['external_url'],\n",
    "            'biography' : self.remove_emojis(parsed['entry_data']['ProfilePage'][0]['graphql']['user']['biography'].replace('\\n', ' ')),\n",
    "            'business_category_name': parsed['entry_data']['ProfilePage'][0]['graphql']['user']['business_category_name'],\n",
    "            'business_email': parsed['entry_data']['ProfilePage'][0]['graphql']['user']['business_email'],\n",
    "            'following' : parsed['entry_data']['ProfilePage'][0]['graphql']['user']['edge_follow']['count'],\n",
    "            'followers' : parsed['entry_data']['ProfilePage'][0]['graphql']['user']['edge_followed_by']['count'],\n",
    "            'posts'     : parsed['entry_data']['ProfilePage'][0]['graphql']['user']['edge_owner_to_timeline_media']['count']\n",
    "        }\n",
    "        \n",
    "        return(metadata)\n",
    "        \n",
    "    # Load all images through infinite scroll and grab each image's link and basic information\n",
    "    def get_posts(self, metadata):\n",
    "        # Since Instagram hides posts from the DOM tree as they disappear from view during scroll, we have to iterate over screens\n",
    "        # The number of iterations = (total number of posts - the initial 12 images) / by the average of 10 images per scroll\n",
    "        n = int((metadata['posts']-12)/10)\n",
    "        posts = list()\n",
    "        n = 2\n",
    "        for i in range(n):\n",
    "            # Scroll to the bottom of the page\n",
    "            self.driver.find_element_by_tag_name('body').send_keys(Keys.END)\n",
    "            \n",
    "            # Find all posts visible in the DOM tree\n",
    "            post_urls = self.driver.find_elements_by_xpath(\"//a[starts-with(@href,'/p/')]\")\n",
    "            \n",
    "            for url in post_urls:\n",
    "                # Add to list\n",
    "                posts.append({\n",
    "                    'date'    : self.today,\n",
    "                    'username': metadata['username'],\n",
    "                    'post_url': url.get_attribute('href')\n",
    "                })\n",
    "            \n",
    "            # Randomized delay between requests\n",
    "            time.sleep(random.randint(1,3))\n",
    "        \n",
    "        return(posts)\n",
    "    \n",
    "    def get_post_metadata(self, posts):\n",
    "        user_posts = list()\n",
    "\n",
    "        for one in posts:\n",
    "            self.driver.get(one['post_url'])\n",
    "            \n",
    "            # Parse the page JSON into a dictionary\n",
    "            parsed = self.parse_json(self.driver.page_source)\n",
    "\n",
    "            metadata = {\n",
    "                'date'      : self.today,\n",
    "                'username'  : one['username'],\n",
    "                'post_url'  : one['post_url'],\n",
    "                'auto_generated_desc': parsed['entry_data']['PostPage'][0]['graphql']['shortcode_media']['accessibility_caption'],\n",
    "                'is_video'  : 1 if parsed['entry_data']['PostPage'][0]['graphql']['shortcode_media']['is_video'] else 0,\n",
    "                'caption'   : self.remove_emojis(parsed['entry_data']['PostPage'][0]['graphql']['shortcode_media']['edge_media_to_caption']['edges'][0]['node']['text']),\n",
    "                'likes'     : parsed['entry_data']['PostPage'][0]['graphql']['shortcode_media']['edge_media_preview_like']['count'],\n",
    "                'comments'  : parsed['entry_data']['PostPage'][0]['graphql']['shortcode_media']['edge_media_to_comment']['count'],\n",
    "                'location'  : parsed['entry_data']['PostPage'][0]['graphql']['shortcode_media']['location']['name']\n",
    "                # can add tagged users here, if needed\n",
    "            }\n",
    "            \n",
    "            user_posts.append(metadata)\n",
    "            \n",
    "            # Randomized delay between requests\n",
    "            time.sleep(random.randint(2,3))\n",
    "        \n",
    "        df = pd.DataFrame(user_posts)\n",
    "        display(df)\n",
    "        df.to_csv('account_posts/{0}.csv'.format(self.today.replace('-', '')), index=False)\n",
    "\n",
    "    # Load all followers through infinite scroll and grab their usernames\n",
    "    def get_followers(self, metadata):\n",
    "        # Find the Followers link and click it\n",
    "        self.driver.find_element_by_partial_link_text(\"follower\").click()\n",
    "        \n",
    "        # Wait a bit for the modal to fully load\n",
    "        time.sleep(3)\n",
    "        \n",
    "        # The first scroll only happens around this line, not at the bottom of the modal\n",
    "        scroll = self.driver.find_element_by_xpath(\"//*[contains(text(), 'Suggestions For You')]\")\n",
    "        scroll.location_once_scrolled_into_view\n",
    "        \n",
    "        # The number of iterations = (total number of followers - the initial 12) / by the 12 followers per scroll\n",
    "        n = int((metadata['followers']-12)/12)\n",
    "        followers = list()\n",
    "        \n",
    "        # Look inside the modal window only\n",
    "        modal = self.driver.find_element_by_xpath(\"//div[@role='dialog']\")        \n",
    "        n=2\n",
    "        for i in range(n):\n",
    "            # Then just find the last Follow button\n",
    "            last_element = modal.find_elements_by_xpath(\"//div[@role='button']\")[-1]\n",
    "            last_element.location_once_scrolled_into_view\n",
    "\n",
    "            # Delay between requests\n",
    "            time.sleep(1)\n",
    "\n",
    "        # Get all the links\n",
    "        links = modal.find_elements_by_tag_name('a')\n",
    "        lst = list()\n",
    "\n",
    "        for i in links:\n",
    "            lst.append(i.get_attribute('href'))\n",
    "\n",
    "        # Remove duplicates, as every item has two '<a>' tags (redirect link and follow button)\n",
    "        lst = list(dict.fromkeys(lst))\n",
    "\n",
    "        followers += lst\n",
    "        \n",
    "        result = {\n",
    "            'username': metadata['username'],\n",
    "            'followers': followers\n",
    "        }\n",
    "        \n",
    "        return(result)\n",
    "        \n",
    "    # Placeholder for retreiving comments\n",
    "    def get_comments(self, post):\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py 3.7 (scrape)",
   "language": "python",
   "name": "scrape"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
